{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MACHINE LEARNING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1: How to define/select metrics?\n",
    "- There isn’t a one-size-fits-all metric. The metric(s) chosen to evaluate a machine learning model depends on various factors:\n",
    "    - Is it a regression or classification task?\n",
    "    - What is the business objective? E.g. Precision vs Recall\n",
    "    - What is the distribution of the target variable?\n",
    "- There are a number of metrics that can be used, including Adjusted R-Squared, MAE, MSE, RMSE, accuracy, recall, precision, f1 score, MCC, ROC-AUC score and the list goes on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2: How to deal with unbalanced binary classification?\n",
    "- You can improve the balance of classes through oversampling the minority class (e.g. SMOTE Resampling) or by undersampling the majority class (e.g. Tomek links)\n",
    "- Give attention to more relevant metrics such as Recall/Sensitivity (to reduce False Negatives) & Precision/Specificity (to reduce False Positives), not only relying on Accuracy score \n",
    "- Use machine learning models that are more robust against imbalances such as XGBoost "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3: What is the difference between a box plot and a histogram?\n",
    "- Histogram is usually used to approximate the probability distribution of the given variable & its distribution shape\n",
    "- Boxplot is usually used for observing the data range, quartiles & its outliers. It is useful if you want to compare multiple continuous charts at the same time for they take less space than histograms in doing the same thing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4: Describe different regularization methods, such as L1 and L2 regularization?\n",
    "- Both L1 (Lasso Regression) and L2 (Ridge Regression) regularization are methods used to reduce the overfitting of training data. Both works well for feature selection when there's too much feature to begin with.\n",
    "- While L2 penalizes the less important features' coefficient, the L1 shrinks the less important feature’s coefficient to zero thus, removing features altogether. \n",
    "- L2 is less robust but has a more stable solution. L1 is more robust but has a more unstable solution, possibly multiple solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5: What is a Neural Network?\n",
    "- A neural network is a multi-layered model inspired by the human brain. In more practical terms, neural networks are non-linear statistical data modeling or decision making tools. It consist of input layers, hidden layers and output layers. Each node in the hidden layers represents a function that the inputs go through, ultimately leading to the output layer. All inputs are modified by a weight and summed, with positive weight reflects an excitatory connection, while negative values mean inhibitory connections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6: What is cross-validation?\n",
    "- A technique used to assess how well a model performs on a new independent dataset. The simplest example of cross-validation is when you split your data into two groups: training data and testing data, where you use the training data to build the model and the testing data to test the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q7: What is Recall & Precision?\n",
    "- Recall attempts to answer “What proportion of actual positives was identified correctly?”\n",
    "    - It seeks to minimize False Negative\n",
    "    - [ True Positives/(True Positives + False Negatives) ]\n",
    "- Precision attempts to answer “What proportion of positive identifications was actually correct?”\n",
    "    - It seeks to minimize False Positive\n",
    "    - [ True Positives/(True Positives + False Positives) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q8: What is False Positive & False Negative?\n",
    "- False Positive (FP) is an incorrect identification of the presence of a condition when it’s actually absent, also known as Type 1 Error\n",
    "- False Negative (FN) is an incorrect identification of the absence of a condition when it’s actually present, also known as Type 2 Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q9: What's the difference between supervised learning and unsupervised learning?\n",
    "- Supervised learning involves learning a function that maps an output based on assigned labels (e.g. predicting default or not)\n",
    "    - labeled training data needed\n",
    "- Unsupervised learning involves learning a function that maps an output without references to labeled results (e.g. clustering customer data)\n",
    "    - no labeled training data needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q10 What is an Adjusted R-Squared?\n",
    "- R-Squared measures the proportion of the variation in your dependent variable (y) explained by your independent variable (x) for a linear regression model. \n",
    "- Adjusted R-Squared adjusts the previous statistic based on the number of independent variables in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q11 What are the advantages of dimension reduction?\n",
    "- It reduces the time and storage space required\n",
    "- It becomes easier to visualize the data when reduced to very low dimensions such as 2D or 3D\n",
    "- It avoids the curse of dimensionality \n",
    "- It removes multi-collinearity & improves the interpretation of the parameters of the machine learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q12 What is Principal Component Analysis (PCA)?\n",
    "- In practical terms, PCA is a tool for decomposing/reducing number of features while keeping all original variables in the model.\n",
    "- It's commonly used for compression purposes, to reduce required memory and to speed up the algorithm, as well as for visualization purposes, making it easier to summarize data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q13 What is the drawback of using Naive Bayes?\n",
    "- One major drawback of Naive Bayes is that it holds a strong assumption in that the features are assumed to be uncorrelated with one another, which typically is never the case. One way to improve such an algorithm that uses Naive Bayes is by decorrelating the features so that the assumption holds true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q14 What is the drawback of using linear models?\n",
    "- The major drawback of linear models is that it holds a strong assumption on multivariate normality, linear relationship, no autocorrelation, no heteroscedasticity & no/little multicollinearity, which typically is never the case.\n",
    "- Extreme violations of these assumptions will make the results redundant. Small violations of these assumptions will result in a greater bias or variance of the estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q15 What is multicollinearity and what to do with it?\n",
    "- Multicollinearity exists when an independent variable is highly correlated with another independent variable in a multiple regression equation. This can be problematic because it undermines the statistical significance of an independent variable.\n",
    "- You could use the Variance Inflation Factors (VIF) to determine if there is any multicollinearity between independent variables. Standard benchmark is that if the VIF > 5 then multicollinearity exists while VIF <= 10 is still acceptable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q16: Why is MSE a bad measure of model performance? What would you suggest instead?\n",
    "- Mean Squared Error (MSE) gives a relatively high weight to large errors, therefore, MSE tends to put too much emphasis on large deviations.\n",
    "- A more robust alternative is Root Mean Squared Error (RMSE) where the large penalized errors are taken into account while given a more balanced emphasis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q17: How to check if the regression model fits the data well?\n",
    "- RMSE: Absolute measure of fit.\n",
    "- R-squared/Adjusted R-squared: Relative measure of fit.\n",
    "- F1 Score: Evaluates the null hypothesis that all regression coefficients are equal to zero vs the alternative hypothesis that at least one doesn’t equal zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q18: What is a decision tree?\n",
    "- Decision tree is a popular model, used both in Regression & Classification problems\n",
    "- Each square above is called a node and starts with a root node. The more nodes you have, the more accurate your decision tree will be (generally). \n",
    "- The last nodes of the decision tree (where a decision is made), are called the leaves of the tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q19: What is a random forest? Why is it good?\n",
    "- Random forests are an ensemble learning technique that builds off of multiple decision trees. \n",
    "- Random forests involve creating multiple decision trees using bootstrapped datasets of the original data and randomly selecting a subset of variables at each step of the decision tree. \n",
    "- The model then selects the mode of all of the predictions of each decision tree. By relying on a “majority wins” model, it reduces the risk of error & bias from an individual tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q20: What is overfitting?\n",
    "- Overfitting is an error where the model ‘fits’ the data too well, resulting in a model with high variance and low bias. \n",
    "- As a consequence, an overfit model will inaccurately predict new data points even though it has a high accuracy on the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q21: What is underfitting?\n",
    "- Underfitting is an error where the model ‘fits’ the data too poorly, resulting in a model with low variance and high bias. \n",
    "- As a consequence, an underfit model will have low accuracy both in training & test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q22: What is boosting?\n",
    "- Boosting is an iterative techniqu which adjust the weight of an observation based on the last classificiation.\n",
    "- Its an ensemble method to improve a model by reducing its bias and variance, ultimately converting weak learners to strong learners. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q23: What is Bias?\n",
    "- IN ML: Bias is an error introduced to our model due to the oversimplification of machine learning algorithm that can lead to underfitting\n",
    "- IN STATISTICS: Bias a tendency of sample statistics to systematically over or underestimate population parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q24: What is Variance?\n",
    "- IN ML: Variance is an error introduced to our model due to the complexity of machine learning algorithm that can lead to overfitting\n",
    "- IN STATISTICS: Variance is a measure of how far a set of numbers is spread out from their average value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q25: What is a Confusion Matrix?\n",
    "- Its a 2x2 matrix that consisted of 4 outputs provide by the binary classifier\n",
    "- It is where we get info of TP, TN, FP & FN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q26: What is a ROC-AUC Curve?\n",
    "- Its a performance measurement for the classification problems at various threshold settings\n",
    "- ROC (Receiver Operating Characteristics) is the porbability curve\n",
    "- AUC (Area Under the Curve) is the degree of separability\n",
    "    - AUC 1 means the model has a 100% probability to distinguish between classes\n",
    "    - AUC 0.7 means the model has a 70% probability to distinguish between classes\n",
    "    - AUC 0.5 means the model is unable to differentiate between classes (50-50)\n",
    "    - AUC 0 means the model predicts the 1 as 0 and vice-versa\n",
    "- TPR (True Positive Rate/Recall/Sensitivity) = TP/TP+FN\n",
    "- FPR (False Positive Rate) = FP/FP+TN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STATISTICS & PROBABILITY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1: What are the fundamentals of probability?\n",
    "- Rule #1: For any event A, 0 ≤ P(A) ≤ 1; in other words, the probability of an event can range from 0 to 1\n",
    "- Rule #2: The sum of the probabilities of all possible outcomes always equals 1.\n",
    "- Rule #3: P(not A) = 1 — P(A); This rule explains the relationship between the probability of an event and its complement event. A complement event is one that includes all possible outcomes that aren’t in A.\n",
    "- Rule #4: If A and B are disjoint events (mutually exclusive), then P(A or B) = P(A) + P(B); this is called the addition rule for disjoint events\n",
    "- Rule #5: P(A or B) = P(A) + P(B) — P(A and B); this is called the general addition rule.\n",
    "- Rule #6: If A and B are two independent events, then P(A and B) = P(A) * P(B); this is called the multiplication rule for independent events\n",
    "- Rule #7: The conditional probability of event B given event A is P(B|A) = P(A and B) / P(A)\n",
    "- Rule #8: For any two events A and B, P(A and B) = P(A) * P(B|A); this is called the general multiplication rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2: Fundamental Counting Principle (multiplication)\n",
    "This method should be used when repetitions are allowed and the number of ways to fill an open place is not affected by previous fills.\n",
    "- e.g. There are 3 types of breakfasts, 4 types of lunches, and 5 types of desserts. \n",
    "    - The total number of combinations is = 5 x 4 x 3 = 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3: Combinations Formula: C(n,r)=(n!)/[(n−r)!r!]\n",
    "This is used when replacements are not allowed and the order in which items are ranked does not mater.\n",
    "- e.g. To win the lottery, you must select the 5 correct numbers in any order from 1 to 52. \n",
    "    - What is the number of possible combinations?\n",
    "C(n,r) = 52! / (52–5)!5! = 2,598,960\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4: Permutations: P(n,r)= n! / (n−r)!\n",
    "This method is used when replacements are not allowed and order of item ranking matters.\n",
    "- e.g. A code has 4 digits in a particular order and the digits range from 0 to 9. \n",
    "    - How many permutations are there if one digit can only be used once?\n",
    "P(n,r) = 10!/(10–4)! = (10x9x8x7x6x5x4x3x2x1)/(6x5x4x3x2x1) = 5040"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5: Difference between convex and non-convex cost function; what does it mean when a cost function is non-convex?\n",
    "- A convex function is one where a line drawn between any two points on the graph lies on or above the graph. It has one minimum.\n",
    "- A non-convex function is one where a line drawn between any two points on the graph may intersect other points on the graph. It characterized as “wavy”.\n",
    "- When a cost function is non-convex, it means that there’s a likelihood that the function may find local minima instead of the global minimum, which is typically undesired in machine learning models from an optimization perspective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6: Given two fair dices, what is the probability of getting scores that sum to 4? to 8? how about getting sum 4 or 8?\n",
    "- The total sample space is of 36 (6 x 6)\n",
    "- There are 3 combinations of rolling a 4 (1+3, 3+1, 2+2):\n",
    "    - P(rolling a 4) = 3/36 = 1/12\n",
    "- There are 5 combinations of rolling an 8 (2+6, 6+2, 3+5, 5+3, 4+4):\n",
    "    - P(rolling an 8) = 5/36\n",
    "- Getting the sum=4 or sum=8 means adding the probability of each: \n",
    "    - P(rolling a 4 or 8) = 3/36 + 5/36 = 8/36"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q7: You are about to get on a plane to London, you want to know whether you have to bring an umbrella or not. You call three of your random friends and ask each one of them if it’s raining. The probability that your friend is telling the truth is 2/3 and the probability that they are playing a prank on you by lying is 1/3. If all 3 of them tell that it is raining, then what is the probability that it is actually raining in London.\n",
    "- You can tell that this question is related to Bayesian theory because of the last statement which essentially follows the structure, “What is the probability A is true given B is true?” Therefore we need to know the probability of it raining in London on a given day. Let’s assume it’s 25%.\n",
    "- P(A) = probability of it raining = 25%\n",
    "- P(B) = probability of all 3 friends say that it’s raining\n",
    "- P(A|B) probability that it’s raining given they’re telling that it is raining\n",
    "- P(B|A) probability that all 3 friends say that it’s raining given it’s raining = (2/3)³ = 8/27\n",
    "\n",
    "\n",
    "- Step 1: Solve for P(B)\n",
    "    - P(A|B) = P(B|A) * P(A) / P(B), can be rewritten as\n",
    "    - P(B) = P(B|A) * P(A) + P(B|not A) * P(not A)\n",
    "    - P(B) = (2/3)³ * 0.25 + (1/3)³ * 0.75 = 0.25*8/27 + 0.75*1/27\n",
    "- Step 2: Solve for P(A|B)\n",
    "    - P(A|B) = 0.25 * (8/27) / ( 0.25*8/27 + 0.75*1/27)\n",
    "    - P(A|B) = 8 / (8 + 3) = 8/11\n",
    "- Therefore, if all three friends say that it’s raining, then there’s an 8/11 chance that it’s actually raining."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q8: You are given 40 cards with four different colors- 10 Green cards, 10 Red Cards, 10 Blue cards, and 10 Yellow cards. The cards of each color are numbered from one to ten. Two cards are picked at random. Find out the probability that the cards picked are not of the same number and same color.\n",
    "- Since these events are not independent, we can use the rule:\n",
    "- P(A and B) = P(A) * P(B|A) ,which is also equal to\n",
    "- P(not A and not B) = P(not A) * P(not B | not A)\n",
    "- For example:\n",
    "    - P(not 4 and not yellow) = P(not 4) * P(not yellow | not 4)\n",
    "    - P(not 4 and not yellow) = (36/39) * (27/36)\n",
    "    - P(not 4 and not yellow) = 0.692\n",
    "- Therefore, the probability that the cards picked are not the same number and the same color is 69.2%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q9: What is 80/20 rule?: \n",
    "- also known as the Pareto principle; states that 80% of the effects come from 20% of the causes e.g. 80% of sales come from 20% of customers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q10: What is the Law of Large Numbers?\n",
    "- The Law of Large Numbers is a theory that states that as the number of trials increases, the average of the result will become closer to the expected value.\n",
    "- e.g. flipping heads from fair coin 100,000 times should be closer to 0.5 than 100 times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q11: Explain what a long-tailed distribution is and provide three examples of relevant phenomena that have long tails. Why are they important in classification and regression problems?\n",
    "- A long-tailed distribution is a type of heavy-tailed distribution that has a tail (or tails) that drop off gradually and asymptotically.\n",
    "- 3 practical examples include the power law, the Pareto principle (more commonly known as the 80–20 rule), and product sales (i.e. best selling products vs others).\n",
    "- It’s important to be mindful of long-tailed distributions in classification and regression problems because the least frequently occurring values make up the majority of the population. This can ultimately change the way that you deal with outliers, and it also conflicts with some machine learning techniques with the assumption that the data is normally distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q12: What is an outlier?\n",
    "- its an extreme value that differs significantly from other observations\n",
    "- its an outlier if its below Q1 - 1.5IQR or above Q3 + 1.5IQR\n",
    "- The use of median instead of mean is better for the measure of central location when outliers are apparent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q13: What is statistical power?\n",
    "\n",
    "- It refers to the power of a binary hypothesis.\n",
    "- Its the probability that the test rejects the null hypothesis given that the alternative is true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q14: What is A/B testing?\n",
    "- A/B testing is a form of hypothesis testing, usually two-sample hypothesis testing to compare two versions, the control and variant, of a single variable. It is commonly used to improve and optimize user experience and marketing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q15: How can you tell if a given coin is biased?\n",
    "The answer is simply to perform a hypothesis test:\n",
    "- The null hypothesis is that the coin is not biased and the probability of flipping heads should equal 50% (H0: p=0.5) \n",
    "- The alternative hypothesis is that the coin is biased (H1: p != 0.5)\n",
    "- Flip the coin 500 times.\n",
    "- Calculate Z-score (if the sample is less than 30, you would calculate the t-statistics).\n",
    "- Compare against alpha (two-tailed test so 0.05/2 = 0.025).\n",
    "- If p-value > alpha, the null is not rejected and the coin is not biased.\n",
    "- If p-value < alpha, the null is rejected and the coin is biased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q16: How do you prove that males are on average taller than females by knowing just gender height?\n",
    "- You can use hypothesis testing to prove that males are taller on average than females.\n",
    "- The null hypothesis would state that males and females are the same height on average, while the alternative hypothesis would state that the average height of males is greater than the average height of females.\n",
    "- Then you would collect a random sample of heights of males and females and use a t-test to determine if you reject the null or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q17: You roll a biased coin (p(head)=0.8) five times. What’s the probability of getting three or more heads?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9488"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.stats as st\n",
    "\n",
    "1 - st.binom.pmf(k=2, n=5, p=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- there's 94% probability of getting three or more heads "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q18: A random variable X is normal with mean 1020 and a standard deviation 50. Calculate P(X>1200)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00015910859015755285"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - st.norm.cdf(1200, loc=1020, scale=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- there's 0.0159% probability of getting X more than 1200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q19: You are running for office and your pollster polled hundred people. Sixty of them claimed they will vote for you. Can you relax?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5039817664728937, 0.6960182335271062)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confidence interval 95%\n",
    "import statsmodels.api as sm\n",
    "sm.stats.proportion_confint(nobs=100, count=60, alpha=0.05) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the upper bounds are satisfactory but the lower bounds are not enough for me to relax (because its only 50%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q20: Consider the number of people that show up at a bus station is Poisson with mean 2.5/h. What is the probability that at most three people show up in a four hour period?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.010336050675925726"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st.poisson.cdf(3, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- probability at most 3 is 1,03%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Probabilities:\n",
      "for 0 people coming in a minute, the probability is:  4.5399929762484854e-05\n",
      "for 1 people coming in a minute, the probability is:  0.0004539992976248486\n",
      "for 2 people coming in a minute, the probability is:  0.0022699964881242435\n",
      "for 3 people coming in a minute, the probability is:  0.007566654960414144\n",
      "for 4 people coming in a minute, the probability is:  0.01891663740103538\n"
     ]
    }
   ],
   "source": [
    "# rate of 10 in 4 hrs\n",
    "rate = 10\n",
    "\n",
    "# guess range\n",
    "import numpy as np\n",
    "n = np.arange(5)\n",
    "\n",
    "mypoisson = st.poisson.pmf(n, rate)\n",
    "print('### Probabilities:')\n",
    "for i,j in list(zip(n,mypoisson)):\n",
    "    print(\"for\",i,'people coming in a minute, the probability is: ',j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q21: Geiger counter records 100 radioactive decays in 5 minutes. Find an approximate 95% interval for the number of decays per hour.\n",
    "- Since this is a Poisson distribution question, mean = lambda = variance, which also means that standard deviation = square root of the mean\n",
    "- a 95% confidence interval implies a z score of 1.96\n",
    "- one standard deviation = 10\n",
    "- Therefore the confidence interval = 100 +/- 19.6 = [964.8, 1435.2]\n",
    "\n",
    "### Q22: The homicide rate in Scotland fell last year to 99 from 115 the year before. Is this reported change really noteworthy?\n",
    "- Since this is a Poisson distribution question, mean = lambda = variance, which also means that standard deviation = square root of the mean\n",
    "- a 95% confidence interval implies a z score of 1.96\n",
    "- one standard deviation = sqrt(115) = 10.724\n",
    "- Therefore the confidence interval = 115+/- 21.45 = [93.55, 136.45]. Since 99 is within this confidence interval, we can assume that this change is not very noteworthy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
